# .github/workflows/reusable-databricks.yml

name: Reusable Databricks CI/CD AWS Workflow

on:
  workflow_call:
    inputs:
      environment:
        required: true
        type: string
      notebook_path:
        required: true
        type: string
      databricks_path:
        required: true
        type: string
    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install Databricks CLI
        run: pip install databricks-cli --upgrade  # Ensure the latest version


      - name: Configure Databricks CLI
        run: |
          mkdir -p ~/.databricks-cli  # Ensure the directory exists
          echo "[DEFAULT]" > ~/.databricks-cli/config
          echo "host = ${{ secrets.DATABRICKS_HOST }}" >> ~/.databricks-cli/config
          echo "token = ${{ secrets.DATABRICKS_TOKEN }}" >> ~/.databricks-cli/config
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

      # - name: Upload notebook to Databricks ${{ inputs.environment }}
      #   run: |
      #     databricks workspace import ${{ inputs.notebook_path }} ${{ inputs.databricks_path }} --language PYTHON --overwrite
      #   env:
      #     DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      #     DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

      - name: Upload notebooks to Databricks
        run: |
          for notebook in $(find notebooks/** -type f \( -iname "*.dbc" -o -iname "*.py" -o -iname "*.ipynb" \)); do
            # Upload each notebook to the specified Databricks path
            databricks workspace import $notebook /Shared/folder/$(basename $notebook) --language PYTHON --overwrite
          done

        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}